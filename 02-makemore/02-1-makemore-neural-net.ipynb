{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d235a7f1-5bd2-4966-ac40-eb286c9fffa6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7acb7193-ccae-448c-9cce-a204969e45cf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# stuff copied from part 1\n",
    "words = open('_resources/names.txt', 'r').read().splitlines()\n",
    "\n",
    "b = {}\n",
    "for w in words:\n",
    "    chs = ['<S>'] + list(w) + ['<E>']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        bigram = (ch1, ch2)\n",
    "        b[bigram] = b.get(bigram, 0) + 1\n",
    "        # print(ch1, ch2)\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "\n",
    "N = torch.zeros((27, 27), dtype=torch.int32) # a tensor with shape (27,27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "a42a2df6-0200-4a9f-abb7-62a2c4a56283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training set of all the bigrams\n",
    "\n",
    "# xs: inputs\n",
    "# ys: targets (labels - the correct next character in a sequence)\n",
    "# => xy is a bigram\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words[:1]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "xs = torch.tensor(xs) # for ['emma']: [ 0,  5, 13, 13,  1]\n",
    "ys = torch.tensor(ys) # for ['emma']: [ 5, 13, 13,  1,  0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b86594a7-93e0-49a6-b3e6-d0091ed21ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0,  5, 13, 13,  1]), tensor([ 5, 13, 13,  1,  0]))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xs,ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6451ed3c-2240-4eac-8ad3-2924a2f60fa3",
   "metadata": {},
   "source": [
    "that up there is integers. we don't want to put those in the NN because it's multiplicative. so z=26 would be more than a=1\n",
    "\n",
    "common solution: \"One Hot Encoding\"\n",
    "- a vector that is all 0s except for the nth dimension, which we turn into 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "4de955b5-a41e-4ef7-bc2e-260430a47ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x encoded\n",
    "# one-hot: `b` is [0,1,0,...] etc (or [0,0,1,0,...] if the first character is start/end\n",
    "# this matrix multiplication _feels_ expensive. but it's actually cheap\n",
    "# because all elements but one are zero. so most multiplications can be skipped!\n",
    "# (modern models use embeddings instead of one-hot encodings)\n",
    "\n",
    "# ðŸš¨ we repeat this next line further down. here just using it to show what xenc looks like\n",
    "xenc = F.one_hot(xs, num_classes=27).float() # need to cast to float because we don't want ints in a neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7be7aa2e-384f-4596-ba64-8bd7c003d038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a6d74392-e0d8-4eef-94fb-8d015fc2b784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x14f9f8a50>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c592793c-8a7a-4bef-9a25-777df8477766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "8b5decde-e199-4c0c-86ef-b66b3357169e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5674],\n",
       "        [-0.0296],\n",
       "        [ 0.0380],\n",
       "        [ 0.0380],\n",
       "        [-0.2373]])"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# neuron: wx+b\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "W = torch.randn((27, 1), generator=g) # normal dist, column vector, [27,1]\n",
    "\n",
    "# @ is matrix multiplication operator in pytorch\n",
    "# xenc is [5,27], w is [27,1]\n",
    "# matrix multiplication: inner dimensions (27 in this case) need to match\n",
    "# example:\n",
    "#\n",
    "# A = [[1,2,3,4],\n",
    "#      [5,6,7,8]]\n",
    "#\n",
    "# B = [[1,2,3],   or [[a,b,c],\n",
    "#      [4,5,6],       [d,e,f],\n",
    "#      [7,8,9],       [g,h,i],\n",
    "#      [10,11,12]]    [j,k,l]]\n",
    "#\n",
    "# (2,4)@(4,3) => 2,3\n",
    "#\n",
    "# C(1,1) = 1a+2d+3g+4j C(1,2) = 1b+2e+3h+4k C(1,3) = 1c+2f+3i+4l\n",
    "# C(2,1) = 5a+6d+7g+8j C(2,2) = 5b+6e+7h+8k C(2,3) = 5c+6f+7i+8l\n",
    "#\n",
    "# C(1,1) = 1*1+2*4+3*7+4*10=70\n",
    "# C(2,3) = 5*3+6*6+7*9+8*12=210\n",
    "# etc...\n",
    "#\n",
    "# C = [[70,80,90],[158,184,210]]\n",
    "r = xenc @ W\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "be3185fe-65ee-498c-96dd-4c806a4741be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸš¨ why does this reuse lines from above? => oh, because above was demo for a single neuron\n",
    "\n",
    "# instead of a single neuron, we want 27 neurons\n",
    "# and in parallel evaluate all 27 neurons on all 5 inputs\n",
    "# (5,27) @ (27,27) => (5,27)\n",
    "# for each of the 27 neurons, what is the firing rate of those neurons\n",
    "# on each of those 5 examples\n",
    "# so r[3,13] gives us the firing rate of the 13th neuron looking at the 3rd input\n",
    "# \n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27,27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee77360c-6c7e-4b28-b10c-a9a6907e4f9a",
   "metadata": {},
   "source": [
    "![neural net](./_resources/neural_net2.jpg)\n",
    "\n",
    "We now have have\n",
    "- 27 inputs (xenc) => input layer\n",
    "- 27 neurons (they perform W*x, weights are W, no bias, no nonlinearity like tanh) => hidden layer\n",
    "- we don't have a second hidden layer in this case\n",
    "\n",
    "Now let's think about what the outputs should be\n",
    "- what we're trying to produce is: for every input sample, a probability distribution for the next character in the distribution...\n",
    "- ...but how will we interpret those 27 output numbers?\n",
    "- we're using a normal distribution, so some are negative and some are positive. but we want them to represent probabilities which are 0-1 (and sum to 1)\n",
    "- what the 27 output numbers are giving us is log counts\n",
    "- to get the counts, we take the log counts and exponentiate them: x => e^x\n",
    "- e^x: for negative numbers, you get <1, for positive numbers you get >1 until infinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "cfbd248e-9938-4e2d-bcc2-a5c7196a30d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
       "         0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
       "         0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459],\n",
       "        [0.0290, 0.0796, 0.0248, 0.0521, 0.1989, 0.0289, 0.0094, 0.0335, 0.0097,\n",
       "         0.0301, 0.0702, 0.0228, 0.0115, 0.0181, 0.0108, 0.0315, 0.0291, 0.0045,\n",
       "         0.0916, 0.0215, 0.0486, 0.0300, 0.0501, 0.0027, 0.0118, 0.0022, 0.0472],\n",
       "        [0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
       "         0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
       "         0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126],\n",
       "        [0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
       "         0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
       "         0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126],\n",
       "        [0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n",
       "         0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n",
       "         0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plugging xs into a neural network... this is a forward pass\n",
    "\n",
    "# input to the neural net: one-hot encoding\n",
    "# need to cast to float because we don't want ints in a neural net\n",
    "xenc = F.one_hot(xs, num_classes=27).float() \n",
    "\n",
    "# predict log-counts\n",
    "# logits is a common term for log-counts\n",
    "logits = xenc @ W\n",
    "\n",
    "# counts, equivalent to the `N` array that we used in the bigram example\n",
    "counts = logits.exp() \n",
    "\n",
    "# probabilities for the next character (normalized)\n",
    "probs = counts / counts.sum(1, keepdims=True) \n",
    "\n",
    "# btw: the last 2 lines here are together called a \"softmax\"\n",
    "probs\n",
    "\n",
    "# ðŸš¨ All off these layers are differentiable. So we can backpropagate through them!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cf1591-8b5b-4c38-a0f0-01ce704fe4e9",
   "metadata": {},
   "source": [
    "## softmax\n",
    "\n",
    "a type of normalization function\n",
    "\n",
    "takes logits, exponentiates, divides and normalizes\n",
    "\n",
    "it's a way of taking outputs from a neural network (can be positive or negative) and outputs positive numbers that sum to 1\n",
    "\n",
    "you can put it on top of any other linear layer. and then it makes the neural network output probabilities.\n",
    "\n",
    "![softmax](./_resources/softmax.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1e5a320c-a900-42a4-881a-74c28e9e9748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for every one of our 5 examples (\".emma\"), we now have a row that came out of a neural net\n",
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3fd2a9f4-c254-4119-8003-486c52db0e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
       "        0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
       "        0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we feed a dot into the neural net - get its index, one-hot encode it,\n",
    "# goes into neural net, out comes this distribution of properties\n",
    "\n",
    "# each of these numbers is how likely each of the 27 characters are to come next\n",
    "# as we tune the weights W, the probabilities will change\n",
    "\n",
    "# can we optimize and find a good W so the probabilities that come out are pretty good?\n",
    "# (\"pretty good\" is measured by the loss function)\n",
    "\n",
    "probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "2735b087-d4fd-4d5b-a5de-08c6807379b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "bigram example 1: .e (indexes 0,5)\n",
      "input to the neural net: 0\n",
      "output probabilities from the neural net: tensor([0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
      "        0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
      "        0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "label (actual next character): 5\n",
      "probability assigned by the net to the current character: 0.012286250479519367\n",
      "log likelihood -4.3992743492126465\n",
      "negative log likelihood: 4.3992743492126465\n",
      "-------\n",
      "bigram example 2: em (indexes 5,13)\n",
      "input to the neural net: 5\n",
      "output probabilities from the neural net: tensor([0.0290, 0.0796, 0.0248, 0.0521, 0.1989, 0.0289, 0.0094, 0.0335, 0.0097,\n",
      "        0.0301, 0.0702, 0.0228, 0.0115, 0.0181, 0.0108, 0.0315, 0.0291, 0.0045,\n",
      "        0.0916, 0.0215, 0.0486, 0.0300, 0.0501, 0.0027, 0.0118, 0.0022, 0.0472],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "label (actual next character): 13\n",
      "probability assigned by the net to the current character: 0.018050704151391983\n",
      "log likelihood -4.014570713043213\n",
      "negative log likelihood: 4.014570713043213\n",
      "-------\n",
      "bigram example 3: mm (indexes 13,13)\n",
      "input to the neural net: 13\n",
      "output probabilities from the neural net: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "label (actual next character): 13\n",
      "probability assigned by the net to the current character: 0.026691533625125885\n",
      "log likelihood -3.623408794403076\n",
      "negative log likelihood: 3.623408794403076\n",
      "-------\n",
      "bigram example 4: ma (indexes 13,1)\n",
      "input to the neural net: 13\n",
      "output probabilities from the neural net: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "label (actual next character): 1\n",
      "probability assigned by the net to the current character: 0.07367684692144394\n",
      "log likelihood -2.6080667972564697\n",
      "negative log likelihood: 2.6080667972564697\n",
      "-------\n",
      "bigram example 5: a. (indexes 1,0)\n",
      "input to the neural net: 1\n",
      "output probabilities from the neural net: tensor([0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n",
      "        0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n",
      "        0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "label (actual next character): 0\n",
      "probability assigned by the net to the current character: 0.01497753243893385\n",
      "log likelihood -4.2012038230896\n",
      "negative log likelihood: 4.2012038230896\n",
      "==========\n",
      "average negative log likelihood, i.e. loss = 3.7693049907684326\n"
     ]
    }
   ],
   "source": [
    "# there are five bigrams inside `.emma.`\n",
    "nlls = torch.zeros(5)\n",
    "for i in range(5):\n",
    "    # i-th bigram\n",
    "    x = xs[i].item() # input character index\n",
    "    y = ys[i].item() # label character index\n",
    "    print('-------')\n",
    "    print(f'bigram example {i+1}: {itos[x]}{itos[y]} (indexes {x},{y})')\n",
    "    print('input to the neural net:', x)\n",
    "    print('output probabilities from the neural net:', probs[i])\n",
    "    print('label (actual next character):', y)\n",
    "    p = probs[i, y]\n",
    "    print('probability assigned by the net to the current character:', p.item())\n",
    "    logp = torch.log(p)\n",
    "    print('log likelihood', logp.item())\n",
    "    nll = -logp\n",
    "    print('negative log likelihood:', nll.item())\n",
    "    nlls[i] = nll\n",
    "\n",
    "print('==========')\n",
    "print('average negative log likelihood, i.e. loss =', nlls.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "537e2c99-3a27-4d99-ba2f-f3bac03bffd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.7693, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the random number we started with is not very good. but the solution\n",
    "# is not to look for a better random number.\n",
    "# we have a loss function now!\n",
    "\n",
    "# next: similar to micrograd - convert integers to vectors, run forward pass,\n",
    "# compute loss, backward pass, update\n",
    "\n",
    "# forward pass is further up\n",
    "# probs is our ypred\n",
    "\n",
    "# probs.shape = [5,27] - first index is index into word, 2nd index is letter\n",
    "# probs[0, 5] # 0th index of `.emma.` is `.`, odds that index 5 (`e`) follows it\n",
    "# probs[0, 5], probs[1, 13], probs[2, 13], probs[3, 1], probs[4, 0]\n",
    "# ^ we want to do this but not manually\n",
    "_probs = probs[torch.arange(5), ys]\n",
    "logprobs = _probs.log()\n",
    "avglogprobs = logprobs.mean()\n",
    "loss = -avglogprobs\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "9b27ef82-eacd-4717-838f-b2ce9226cad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0121,  0.0020,  0.0025,  0.0008,  0.0034, -0.1975,  0.0005,  0.0046,\n",
       "          0.0027,  0.0063,  0.0016,  0.0056,  0.0018,  0.0016,  0.0100,  0.0476,\n",
       "          0.0121,  0.0005,  0.0050,  0.0011,  0.0068,  0.0022,  0.0006,  0.0040,\n",
       "          0.0024,  0.0307,  0.0292],\n",
       "        [-0.1970,  0.0017,  0.0079,  0.0020,  0.0121,  0.0062,  0.0217,  0.0026,\n",
       "          0.0025,  0.0010,  0.0205,  0.0017,  0.0198,  0.0022,  0.0046,  0.0041,\n",
       "          0.0082,  0.0016,  0.0180,  0.0106,  0.0093,  0.0062,  0.0010,  0.0066,\n",
       "          0.0131,  0.0101,  0.0018],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0058,  0.0159,  0.0050,  0.0104,  0.0398,  0.0058,  0.0019,  0.0067,\n",
       "          0.0019,  0.0060,  0.0140,  0.0046,  0.0023, -0.1964,  0.0022,  0.0063,\n",
       "          0.0058,  0.0009,  0.0183,  0.0043,  0.0097,  0.0060,  0.0100,  0.0005,\n",
       "          0.0024,  0.0004,  0.0094],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0125, -0.1705,  0.0194,  0.0133,  0.0270,  0.0080,  0.0105,  0.0100,\n",
       "          0.0490,  0.0066,  0.0030,  0.0316,  0.0052, -0.1893,  0.0059,  0.0045,\n",
       "          0.0234,  0.0049,  0.0260,  0.0023,  0.0083,  0.0031,  0.0053,  0.0081,\n",
       "          0.0482,  0.0187,  0.0051],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# backward pass\n",
    "W.grad = None # make sure all gradients are set to 0\n",
    "\n",
    "# like micrograd, pytorch builds a full computational graph\n",
    "# retain_graph lets us run it multiple times\n",
    "loss.backward(retain_graph=True) \n",
    "\n",
    "# now all the intermediate steps are filled in, so we can run:\n",
    "# [27,27] just like W\n",
    "# every element of W.grad tells us the influence of that weight on the loss function\n",
    "# W.grad[0,0] is 0.0121 (positive), so W[0,0] += h would INCREASE the loss\n",
    "W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "3cbe6d06-c154-401c-8c9d-6aa1be71ed91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5637e+00, -2.3789e-01, -2.8124e-02, -1.1010e+00,  2.8487e-01,\n",
       "          2.9619e-02, -1.5472e+00,  6.0350e-01,  7.8314e-02,  9.0275e-01,\n",
       "         -4.7173e-01,  7.8515e-01, -3.2898e-01, -4.3346e-01,  1.3699e+00,\n",
       "          2.9191e+00,  1.5582e+00, -1.6262e+00,  6.7567e-01, -8.4072e-01,\n",
       "          9.8285e-01, -1.4903e-01, -1.4797e+00,  4.4711e-01, -7.1438e-02,\n",
       "          2.4876e+00,  2.4360e+00],\n",
       "        [-6.1096e-01, -1.2204e+00,  3.0077e-01, -1.0731e+00,  7.2398e-01,\n",
       "          4.9266e-02,  1.3030e+00, -8.0299e-01, -8.5117e-01, -1.8070e+00,\n",
       "          1.2462e+00, -1.2261e+00,  1.2106e+00, -9.6545e-01, -2.3350e-01,\n",
       "         -3.4886e-01,  3.2999e-01, -1.3267e+00,  1.1170e+00,  5.9322e-01,\n",
       "          4.5568e-01,  5.2157e-02, -1.7403e+00,  1.1362e-01,  7.9927e-01,\n",
       "          5.3806e-01, -1.1652e+00],\n",
       "        [ 1.4756e-01, -1.0006e+00,  3.8012e-01,  4.7328e-01, -9.1027e-01,\n",
       "         -7.8305e-01,  1.3506e-01, -2.1161e-01, -1.0406e+00, -1.5367e+00,\n",
       "          9.3743e-01, -8.8303e-01,  1.7457e+00,  2.1346e+00, -8.5614e-01,\n",
       "          5.4082e-01,  6.1690e-01,  1.5160e+00, -1.0447e+00, -6.6414e-01,\n",
       "         -7.2390e-01,  1.7507e+00,  1.7530e-01,  9.9280e-01, -6.2787e-01,\n",
       "          7.7023e-02, -1.1641e+00],\n",
       "        [ 1.2473e+00, -2.7061e-01, -1.3635e+00,  1.3066e+00,  3.2307e-01,\n",
       "          1.0358e+00, -8.6249e-01, -1.2575e+00,  9.4180e-01, -1.3257e+00,\n",
       "          1.4670e-01,  1.6913e-01, -1.5397e+00, -7.2759e-01,  1.1491e+00,\n",
       "         -8.7462e-01, -2.9771e-01, -1.3707e+00,  1.1500e-01, -1.0188e+00,\n",
       "         -8.3777e-01, -2.1057e+00, -2.6044e-01, -1.7149e+00, -3.3787e-01,\n",
       "         -1.8263e+00, -8.3897e-01],\n",
       "        [-1.5723e+00,  4.5795e-01, -5.6533e-01,  5.4281e-01,  1.7549e-01,\n",
       "         -2.2901e+00, -7.0928e-01, -2.9284e-01, -2.1803e+00,  7.9311e-02,\n",
       "          9.0187e-01,  1.2028e+00, -5.6144e-01, -1.3753e-01, -1.3799e-01,\n",
       "         -2.0977e+00, -7.9238e-01,  6.0689e-01, -1.4777e+00, -5.1029e-01,\n",
       "          5.6421e-01,  9.6838e-01, -3.1114e-01, -3.0603e-01, -1.7495e+00,\n",
       "         -1.6335e+00,  3.8761e-01],\n",
       "        [ 4.7062e-01,  1.4782e+00,  3.1599e-01,  1.0557e+00,  2.3863e+00,\n",
       "          4.6653e-01, -6.5706e-01,  6.1461e-01, -6.2256e-01,  5.0827e-01,\n",
       "          1.3521e+00,  2.3308e-01, -4.5654e-01,  5.7604e-02, -5.1226e-01,\n",
       "          5.5381e-01,  4.7284e-01, -1.3869e+00,  1.6174e+00,  1.7068e-01,\n",
       "          9.8555e-01,  5.0477e-01,  1.0168e+00, -1.9064e+00, -4.2824e-01,\n",
       "         -2.1260e+00,  9.5757e-01],\n",
       "        [ 1.2482e+00,  2.5341e-01,  2.8188e+00, -3.3983e-01,  7.0311e-01,\n",
       "          4.0716e-01, -1.9018e-01, -6.9652e-01,  1.7039e+00,  7.4204e-01,\n",
       "          9.7370e-01,  3.0028e-01, -2.8971e-01, -3.1566e-01, -8.7898e-01,\n",
       "          1.0661e-01,  1.8598e+00,  5.5752e-02,  1.2815e+00, -6.3182e-01,\n",
       "         -1.2464e+00,  6.8305e-01, -3.9455e-01,  1.4388e-02,  5.7216e-01,\n",
       "          8.6726e-01,  6.3149e-01],\n",
       "        [-1.2230e+00, -2.1286e-01,  5.0950e-01,  3.2713e-01,  1.9661e+00,\n",
       "         -2.4091e-01, -7.9515e-01,  2.7198e-01, -1.1100e+00, -4.5285e-01,\n",
       "         -4.9578e-01,  1.2648e+00,  1.4625e+00,  1.1199e+00,  9.9539e-01,\n",
       "         -1.2353e+00,  7.3818e-01,  8.1415e-01, -7.3806e-01,  5.6714e-01,\n",
       "         -1.4601e+00, -2.4780e-01,  8.8282e-01, -8.1004e-02, -9.5299e-01,\n",
       "         -4.8838e-01, -7.3712e-01],\n",
       "        [ 7.0609e-01, -1.9295e-01,  1.2348e+00,  3.3308e-01,  1.3283e+00,\n",
       "         -1.0921e+00, -8.3952e-01,  1.9098e-01, -7.1750e-01, -3.8668e-01,\n",
       "         -1.2542e+00,  1.2068e+00, -1.7102e+00, -4.7701e-01, -1.0527e+00,\n",
       "         -1.4367e-01, -2.7737e-01,  1.1634e+00, -6.6910e-01,  6.4918e-01,\n",
       "          5.8243e-01,  1.9264e+00, -3.7846e-01,  7.9577e-03,  5.1068e-01,\n",
       "          7.5927e-01, -1.6086e+00],\n",
       "        [-1.6065e-01,  1.3784e+00, -2.7804e-01,  2.0710e-01,  1.0033e+00,\n",
       "         -5.9772e-01, -3.9771e-01, -1.2801e+00,  9.2445e-02,  1.0526e-01,\n",
       "         -3.9072e-01, -4.0091e-01,  5.6533e-01, -1.5065e+00,  1.2898e+00,\n",
       "         -1.5100e+00,  1.0930e+00,  1.0797e+00, -8.6681e-02,  1.3423e+00,\n",
       "          1.5184e-01,  2.4687e-01,  3.1895e-01, -9.8614e-01, -2.1382e-01,\n",
       "         -6.4308e-02, -8.5528e-01],\n",
       "        [ 1.6113e-01,  4.4925e-01,  8.1827e-01, -8.1628e-01, -3.9243e-01,\n",
       "         -7.4521e-01, -9.4649e-01, -1.5941e-01, -1.5047e+00,  8.4682e-01,\n",
       "         -4.9158e-02,  9.3866e-02, -6.4533e-01,  1.2108e+00, -7.8198e-01,\n",
       "          3.8449e-01, -8.5259e-01,  1.0464e+00, -1.8493e+00,  9.1092e-01,\n",
       "         -9.9360e-01,  6.0195e-01, -1.0890e-01,  5.2587e-01, -9.4046e-01,\n",
       "         -1.2773e-01, -2.5679e-01],\n",
       "        [-1.5437e+00,  3.7950e-01, -1.7705e+00, -1.2085e+00,  9.4773e-01,\n",
       "         -9.1355e-01,  7.1023e-01,  7.9512e-01,  5.7662e-01, -7.3778e-01,\n",
       "         -1.5264e+00,  7.1173e-01,  1.4056e+00, -4.0636e-01, -7.4648e-01,\n",
       "          4.9790e-01,  1.1298e-01, -4.1854e-01,  1.7905e-01,  2.3483e-01,\n",
       "          7.3510e-01, -6.1577e-01,  7.0467e-01,  1.1630e-01,  2.8365e-01,\n",
       "         -2.5043e+00, -5.1931e-01],\n",
       "        [-5.9134e-01, -1.1059e-01,  8.3416e-01, -1.0505e+00,  3.6345e-01,\n",
       "          1.8195e-01, -4.8045e-01,  5.3309e-01,  6.7869e-01, -3.5974e-01,\n",
       "         -1.3270e+00, -8.2526e-01,  6.3614e-01,  1.9110e-01,  7.5476e-01,\n",
       "          4.0538e-01,  2.2565e+00,  1.3655e+00, -5.6192e-01, -3.0423e-01,\n",
       "          2.9894e-01,  1.8784e+00,  5.5958e-01,  1.3388e+00,  4.1606e-01,\n",
       "          6.8491e-01, -1.4790e-01],\n",
       "        [ 1.8985e-01,  1.1044e+00,  6.2812e-01,  2.5386e-01,  9.5599e-01,\n",
       "         -2.5095e-01,  2.1595e-02, -3.3395e-02,  1.5475e+00, -4.5049e-01,\n",
       "         -1.2354e+00,  1.1125e+00, -6.7538e-01,  9.4680e-02, -5.6058e-01,\n",
       "         -8.2844e-01,  8.1551e-01, -7.5245e-01,  9.1998e-01, -1.4856e+00,\n",
       "         -2.1542e-01, -1.1870e+00, -6.6251e-01, -2.3592e-01,  1.5303e+00,\n",
       "          5.9499e-01, -7.1060e-01],\n",
       "        [ 1.9217e+00, -1.8182e-01,  1.5220e+00,  5.4644e-01,  4.0859e-01,\n",
       "         -1.9692e+00, -8.9185e-01,  3.2961e-01, -2.5127e-01,  5.5030e-01,\n",
       "         -7.5171e-01, -6.5783e-03, -6.3108e-01,  1.3431e+00,  3.8010e-02,\n",
       "         -7.1654e-01,  1.7206e+00, -5.2149e-01, -2.3248e-01,  1.0774e+00,\n",
       "         -7.6019e-01,  9.0109e-03, -7.9219e-01,  1.2307e+00, -5.2760e-01,\n",
       "         -1.3207e+00, -7.0654e-01],\n",
       "        [-7.7861e-01,  1.2910e+00, -1.5094e+00,  7.4593e-01,  4.8990e-01,\n",
       "         -1.0034e+00,  9.6407e-01,  2.0990e+00, -3.9870e-01, -7.6635e-01,\n",
       "         -2.1007e+00,  1.2331e+00,  7.7481e-01,  2.4311e-01, -2.1322e-01,\n",
       "         -6.9877e-01,  2.0889e-01, -6.2477e-01, -1.0825e-01, -2.1964e+00,\n",
       "          2.7083e-01,  6.1047e-01, -5.8162e-01, -1.7025e+00, -8.0672e-01,\n",
       "         -2.4174e-01,  1.5490e+00],\n",
       "        [-3.4593e-01,  5.4714e-01,  3.1755e-02,  8.1375e-01,  2.6200e-01,\n",
       "         -6.7101e-01,  2.0656e-02,  7.1300e-01, -4.3997e-02, -5.1944e-01,\n",
       "          1.1241e-01, -3.9770e-01, -2.7829e-01, -1.5364e-01, -2.5424e+00,\n",
       "          2.5033e-01,  1.1056e-01, -2.0366e+00, -9.2735e-01, -6.9350e-01,\n",
       "         -5.2788e-01, -8.7438e-01, -1.0102e+00, -1.0522e+00,  1.2348e+00,\n",
       "          2.5907e-02, -9.6676e-01],\n",
       "        [ 1.0904e+00,  5.3966e-01,  6.6741e-01, -2.2316e+00, -1.1603e+00,\n",
       "         -4.2560e-01,  5.9547e-01, -1.0887e+00,  2.4324e-01, -2.1021e+00,\n",
       "         -2.9289e-01, -7.0682e-01,  9.5190e-01, -1.1583e+00, -1.2844e+00,\n",
       "          1.0193e+00,  1.6851e+00,  8.3422e-01,  1.7113e+00,  4.4456e-01,\n",
       "         -7.1861e-01, -7.0343e-01, -7.1332e-01,  9.9760e-01, -6.1980e-01,\n",
       "          1.9522e+00,  1.4311e-01],\n",
       "        [ 1.8765e-01,  7.5974e-01, -2.6387e-01, -7.3048e-01,  6.1955e-01,\n",
       "          3.5577e-02, -7.6460e-02, -1.2306e+00,  1.3419e+00,  1.1878e+00,\n",
       "         -1.0672e+00, -2.1507e+00,  6.7082e-01,  1.1614e+00, -2.4155e-01,\n",
       "          9.5907e-01,  3.8262e-02,  3.9877e-02, -7.7180e-01,  2.9251e-01,\n",
       "         -6.0606e-01, -1.5136e+00, -2.7143e+00, -4.1164e-01, -1.2273e+00,\n",
       "         -4.1746e-01,  1.5021e+00],\n",
       "        [-6.2849e-01, -4.4247e-01,  5.6885e-01,  1.2803e+00, -5.5397e-01,\n",
       "          1.1179e+00, -6.0053e-01, -5.8619e-01, -2.8277e-01,  5.3390e-01,\n",
       "         -9.9388e-01, -1.6996e+00,  1.8362e+00,  4.2016e-01, -6.8729e-01,\n",
       "         -3.5060e-01,  7.5598e-01, -9.3632e-01, -8.4108e-02, -1.6361e+00,\n",
       "          1.0224e+00,  1.0733e+00, -5.7453e-01,  4.9668e-02,  7.2379e-01,\n",
       "          5.9746e-01,  2.6966e+00],\n",
       "        [ 2.7930e+00, -2.2745e+00, -2.3912e-01,  8.7498e-02,  1.4967e+00,\n",
       "         -5.7016e-01, -5.7248e-01,  1.9909e+00, -7.4416e-01,  7.2960e-01,\n",
       "          6.4083e-01,  1.6075e+00, -8.8810e-01,  2.7359e-01, -1.3257e-01,\n",
       "          1.2710e+00,  1.7234e+00,  1.1180e-01,  2.6952e-01,  1.1835e+00,\n",
       "          1.2575e+00,  1.3969e-01,  4.7259e-01,  7.9025e-01,  1.0811e+00,\n",
       "         -9.1965e-01, -4.0503e-01],\n",
       "        [ 4.5696e-01, -5.4184e-01, -2.3025e+00,  2.0127e+00, -4.6452e-01,\n",
       "         -5.8270e-01,  2.0863e+00, -4.7729e-02, -4.4920e-01,  9.5566e-01,\n",
       "         -1.4708e-01, -1.2532e+00, -1.1850e+00,  3.6583e-01, -1.4049e-01,\n",
       "          3.5252e-01, -5.2400e-01, -6.2845e-01, -9.3792e-01,  1.6772e+00,\n",
       "          3.8554e-03, -7.3685e-01, -9.3514e-01,  1.0465e-01, -4.6464e-01,\n",
       "          1.6676e+00,  1.3931e+00],\n",
       "        [ 6.5398e-01, -2.2449e-01,  1.2831e+00, -9.1787e-01, -3.3916e-01,\n",
       "         -1.8058e+00,  6.0518e-01, -5.6252e-01, -7.8933e-01,  1.2767e+00,\n",
       "         -1.0143e+00,  4.1611e-01, -7.5348e-01,  1.7128e+00, -8.7554e-01,\n",
       "          3.9714e-01,  8.4326e-01,  3.7988e-01, -1.1670e+00,  5.5228e-01,\n",
       "         -1.0279e+00, -3.9554e-01, -7.1410e-01, -8.7456e-02, -3.3361e-01,\n",
       "         -1.8798e-01, -1.2647e+00],\n",
       "        [ 2.0021e+00, -2.3470e-01, -1.3765e+00,  9.3426e-01,  1.0880e+00,\n",
       "          1.9179e-01,  3.0114e-01,  8.9896e-01, -8.4454e-01,  2.3267e-01,\n",
       "         -3.9205e-01, -2.5081e-01,  8.7124e-02,  1.3769e+00, -8.3358e-01,\n",
       "         -8.9400e-01,  1.1744e+00, -6.0779e-01, -1.1493e-01, -7.8077e-01,\n",
       "          1.9660e+00,  6.1175e-01,  3.6039e-01, -1.0274e+00,  1.1495e+00,\n",
       "          4.5111e-01,  6.4420e-01],\n",
       "        [ 2.1635e-01, -7.8731e-01, -3.3005e-01,  3.2877e-01, -1.6332e+00,\n",
       "          1.0807e+00,  3.3638e-01,  1.1536e-01,  3.2834e-01,  5.3447e-02,\n",
       "          1.4224e+00, -8.3957e-01, -2.4956e-01, -8.9778e-01, -8.6583e-01,\n",
       "         -1.0786e+00, -1.8384e-01,  7.1622e-01,  1.8175e-01,  1.1053e+00,\n",
       "          1.7003e+00, -1.6965e-01,  1.6293e-01,  1.3413e+00, -2.6301e-01,\n",
       "         -7.5521e-01,  8.1911e-01],\n",
       "        [ 7.4140e-01, -5.8787e-01, -4.6505e-01,  5.3112e-02,  2.2190e+00,\n",
       "         -3.5158e-01,  3.6381e-01,  2.5769e+00,  1.4544e+00, -6.1003e-01,\n",
       "         -5.9961e-01, -5.8392e-01, -1.8104e-02, -9.5177e-01, -9.6400e-01,\n",
       "         -2.8183e-01,  1.0597e+00, -7.2370e-01,  1.4755e-01, -3.2667e-01,\n",
       "          2.4958e+00,  1.1088e+00, -8.5476e-01,  1.8443e+00, -1.3881e-01,\n",
       "          1.3096e+00, -2.5802e-01],\n",
       "        [ 1.0669e+00,  2.1363e-01, -7.6603e-01, -1.6977e+00, -1.5023e-01,\n",
       "         -5.2150e-01, -6.3730e-01,  2.6214e-01,  7.6541e-03,  1.3067e+00,\n",
       "         -6.3482e-01, -1.1042e-04, -6.6158e-01,  1.4723e-01, -6.6036e-02,\n",
       "          5.2851e-01,  5.7950e-01,  2.1438e-01,  9.2200e-01,  5.2919e-01,\n",
       "          7.7070e-01,  4.2899e-01,  3.4330e-01,  2.0698e+00,  1.3405e+00,\n",
       "         -2.1746e-01,  8.6273e-01]])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# update\n",
    "# we don't need to loop over parameters because there is only one parameter\n",
    "W.data += -0.1 * W.grad\n",
    "W.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "acf6f340-05af-4edf-bb46-32c816499347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.709091901779175"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "c290e832-20a4-4fe1-8548-63e01d685990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2995219230651855\n"
     ]
    }
   ],
   "source": [
    "# we can now do gradient descent!\n",
    "\n",
    "# all of this is copy pasted from above!\n",
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "logits = xenc @ W\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "loss = -probs[torch.arange(5), ys].log().mean()\n",
    "print(loss.item())\n",
    "W.grad = None\n",
    "loss.backward()\n",
    "W.data += -0.1 * W.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4647ef03-4383-4cbf-a00d-71716e3b249d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca666241-0e81-4c42-963f-3767d56c1b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('_resources/names.txt', 'r').read().splitlines()\n",
    "\n",
    "b = {}\n",
    "for w in words:\n",
    "    chs = ['<S>'] + list(w) + ['<E>']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        bigram = (ch1, ch2)\n",
    "        b[bigram] = b.get(bigram, 0) + 1\n",
    "        # print(ch1, ch2)\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "\n",
    "N = torch.zeros((27, 27), dtype=torch.int32) # a tensor with shape (27,27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e72a2cfa-4c21-449f-88f4-ca09f6a148e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples: 228146\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset\n",
    "\n",
    "# xs: inputs to the neural net\n",
    "# ys: labels for the correct next character in a sequence\n",
    "xs, ys = [], []\n",
    "\n",
    "# for w in words[:1]:\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "xs = torch.tensor(xs) # for ['emma']: [ 0,  5, 13, 13,  1]\n",
    "ys = torch.tensor(ys) # for ['emma']: [ 5, 13, 13,  1,  0]\n",
    "\n",
    "num = xs.nelement()\n",
    "\n",
    "print('number of examples:', num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bfcbf73a-2c15-4caa-8429-8afe8f25a2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the 'network'\n",
    "\n",
    "# randomly initialize 27 neurons' wights.\n",
    "# each nueron receives 27 inputs\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8c6aae84-682c-408a-b2d1-14cb56bb0ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1080, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(W**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c67d5a21-21bc-4dcf-8dd8-caddd67441b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4623453617095947\n",
      "2.462298631668091\n",
      "2.462252140045166\n",
      "2.4622061252593994\n",
      "2.46216082572937\n",
      "2.462116003036499\n",
      "2.462071418762207\n",
      "2.4620275497436523\n",
      "2.461984157562256\n",
      "2.4619410037994385\n",
      "2.4618983268737793\n",
      "2.4618561267852783\n",
      "2.4618144035339355\n",
      "2.461773157119751\n",
      "2.4617323875427246\n",
      "2.4616920948028564\n",
      "2.4616518020629883\n",
      "2.4616119861602783\n",
      "2.4615726470947266\n",
      "2.461533784866333\n",
      "2.4614953994750977\n",
      "2.4614572525024414\n",
      "2.4614193439483643\n",
      "2.4613821506500244\n",
      "2.4613449573516846\n",
      "2.461308240890503\n",
      "2.4612717628479004\n",
      "2.461236000061035\n",
      "2.46120023727417\n",
      "2.461164712905884\n",
      "2.461129665374756\n",
      "2.461095094680786\n",
      "2.4610607624053955\n",
      "2.461026668548584\n",
      "2.4609928131103516\n",
      "2.4609594345092773\n",
      "2.4609262943267822\n",
      "2.460893392562866\n",
      "2.4608609676361084\n",
      "2.4608287811279297\n",
      "2.460797071456909\n",
      "2.4607648849487305\n",
      "2.460733652114868\n",
      "2.460702896118164\n",
      "2.4606716632843018\n",
      "2.4606411457061768\n",
      "2.460610866546631\n",
      "2.460580587387085\n",
      "2.4605510234832764\n",
      "2.4605214595794678\n",
      "2.4604921340942383\n",
      "2.460463047027588\n",
      "2.4604339599609375\n",
      "2.4604055881500244\n",
      "2.4603774547576904\n",
      "2.4603493213653564\n",
      "2.4603211879730225\n",
      "2.4602935314178467\n",
      "2.46026611328125\n",
      "2.4602391719818115\n",
      "2.460212230682373\n",
      "2.4601855278015137\n",
      "2.4601590633392334\n",
      "2.460132598876953\n",
      "2.460106611251831\n",
      "2.460080623626709\n",
      "2.460055112838745\n",
      "2.460029363632202\n",
      "2.4600043296813965\n",
      "2.459979295730591\n",
      "2.459954261779785\n",
      "2.4599297046661377\n",
      "2.4599051475524902\n",
      "2.459881067276001\n",
      "2.4598565101623535\n",
      "2.4598326683044434\n",
      "2.4598090648651123\n",
      "2.4597854614257812\n",
      "2.4597620964050293\n",
      "2.4597387313842773\n",
      "2.4597158432006836\n",
      "2.45969295501709\n",
      "2.459670305252075\n",
      "2.4596476554870605\n",
      "2.459625244140625\n",
      "2.4596030712127686\n",
      "2.459581136703491\n",
      "2.459559440612793\n",
      "2.4595375061035156\n",
      "2.4595160484313965\n",
      "2.4594943523406982\n",
      "2.4594736099243164\n",
      "2.4594521522521973\n",
      "2.4594314098358154\n",
      "2.4594106674194336\n",
      "2.4593899250030518\n",
      "2.459369421005249\n",
      "2.4593491554260254\n",
      "2.4593288898468018\n",
      "2.4593088626861572\n"
     ]
    }
   ],
   "source": [
    "# Gradient descent\n",
    "\n",
    "for k in range(100):\n",
    "    \n",
    "    # Forward pass: plug all the input examples (`xs`) into a neural net\n",
    "    \n",
    "    # input to the network: one-hot encoding\n",
    "    # [5,27], mostly 0s, a few 1s\n",
    "    xenc = F.one_hot(xs, num_classes=27).float()\n",
    "    # predict log-counts\n",
    "    # xenc is one_hot! multiplying [0,0,0,0,1,0,...] * W actually plucks out the 5th row of W\n",
    "    logits = xenc @ W \n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    # regularization: \"spring force\"... W wants to be 0, and the probabilities want to be uniform (regularization part),\n",
    "    # but they also simultaneously want to match up the probabilities as indicated by the data (everything before regularization)\n",
    "    loss = -probs[torch.arange(num), ys].log().mean() # + 0.01 * (W**2).mean() # regularize the loss\n",
    "    print(loss.item())\n",
    "    # btw: the last 2 lines here together are called a 'softmax'\n",
    "\n",
    "    # Backward pass\n",
    "\n",
    "    W.grad = None # set to zero the gradient\n",
    "    loss.backward()\n",
    "\n",
    "    # Update\n",
    "    W.data += -50 * W.grad\n",
    "\n",
    "# This converges on ~2.47, which is very similar to what we got in the first example by counting\n",
    "# But the advantage of this approach is that it's more flexible\n",
    "# ie in future exercises we will take more and more past characters and feed them into bigger and bigger neural nets,\n",
    "# ...until we arrive at GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b5b0ffd6-c58f-44ce-8724-46aeca0dd7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "bigram example 1: .e (indexes 0,5)\n",
      "input to the neural net: 0\n",
      "output probabilities from the neural net: tensor([0.0621, 0.0106, 0.0131, 0.0045, 0.0178, 0.0284, 0.0029, 0.0244, 0.0146,\n",
      "        0.0328, 0.0084, 0.0292, 0.0097, 0.0088, 0.0516, 0.2141, 0.0618, 0.0027,\n",
      "        0.0262, 0.0058, 0.0354, 0.0116, 0.0031, 0.0209, 0.0125, 0.1468, 0.1401],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "label (actual next character): 5\n",
      "probability assigned by the net to the current character: 0.028360897675156593\n",
      "log likelihood -3.562743902206421\n",
      "negative log likelihood: 3.562743902206421\n",
      "-------\n",
      "bigram example 2: em (indexes 5,13)\n",
      "input to the neural net: 5\n",
      "output probabilities from the neural net: tensor([0.0294, 0.0777, 0.0252, 0.0519, 0.1780, 0.0293, 0.0096, 0.0338, 0.0100,\n",
      "        0.0305, 0.0689, 0.0233, 0.0118, 0.0400, 0.0111, 0.0319, 0.0294, 0.0047,\n",
      "        0.0885, 0.0219, 0.0485, 0.0304, 0.0500, 0.0028, 0.0121, 0.0022, 0.0472],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "label (actual next character): 13\n",
      "probability assigned by the net to the current character: 0.03996948525309563\n",
      "log likelihood -3.2196390628814697\n",
      "negative log likelihood: 3.2196390628814697\n",
      "-------\n",
      "bigram example 3: mm (indexes 13,13)\n",
      "input to the neural net: 13\n",
      "output probabilities from the neural net: tensor([0.0294, 0.1356, 0.0445, 0.0313, 0.0603, 0.0192, 0.0250, 0.0237, 0.1017,\n",
      "        0.0158, 0.0073, 0.0695, 0.0127, 0.0542, 0.0142, 0.0109, 0.0530, 0.0118,\n",
      "        0.0584, 0.0057, 0.0199, 0.0077, 0.0129, 0.0195, 0.1002, 0.0432, 0.0123],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "label (actual next character): 13\n",
      "probability assigned by the net to the current character: 0.05420778691768646\n",
      "log likelihood -2.914930820465088\n",
      "negative log likelihood: 2.914930820465088\n",
      "-------\n",
      "bigram example 4: ma (indexes 13,1)\n",
      "input to the neural net: 13\n",
      "output probabilities from the neural net: tensor([0.0294, 0.1356, 0.0445, 0.0313, 0.0603, 0.0192, 0.0250, 0.0237, 0.1017,\n",
      "        0.0158, 0.0073, 0.0695, 0.0127, 0.0542, 0.0142, 0.0109, 0.0530, 0.0118,\n",
      "        0.0584, 0.0057, 0.0199, 0.0077, 0.0129, 0.0195, 0.1002, 0.0432, 0.0123],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "label (actual next character): 1\n",
      "probability assigned by the net to the current character: 0.13556894659996033\n",
      "log likelihood -1.9982749223709106\n",
      "negative log likelihood: 1.9982749223709106\n",
      "-------\n",
      "bigram example 5: a. (indexes 1,0)\n",
      "input to the neural net: 1\n",
      "output probabilities from the neural net: tensor([0.0331, 0.0089, 0.0396, 0.0102, 0.0596, 0.0310, 0.1030, 0.0134, 0.0128,\n",
      "        0.0049, 0.0977, 0.0088, 0.0945, 0.0114, 0.0235, 0.0210, 0.0408, 0.0080,\n",
      "        0.0866, 0.0526, 0.0461, 0.0311, 0.0053, 0.0330, 0.0641, 0.0499, 0.0094],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "label (actual next character): 0\n",
      "probability assigned by the net to the current character: 0.03310370072722435\n",
      "log likelihood -3.4081101417541504\n",
      "negative log likelihood: 3.4081101417541504\n",
      "==========\n",
      "average negative log likelihood, i.e. loss = 3.0207395553588867\n"
     ]
    }
   ],
   "source": [
    "# there are five bigrams inside `.emma.`\n",
    "nlls = torch.zeros(5)\n",
    "for i in range(5):\n",
    "    # i-th bigram\n",
    "    x = xs[i].item() # input character index\n",
    "    y = ys[i].item() # label character index\n",
    "    print('-------')\n",
    "    print(f'bigram example {i+1}: {itos[x]}{itos[y]} (indexes {x},{y})')\n",
    "    print('input to the neural net:', x)\n",
    "    print('output probabilities from the neural net:', probs[i])\n",
    "    print('label (actual next character):', y)\n",
    "    p = probs[i, y]\n",
    "    print('probability assigned by the net to the current character:', p.item())\n",
    "    logp = torch.log(p)\n",
    "    print('log likelihood', logp.item())\n",
    "    nll = -logp\n",
    "    print('negative log likelihood:', nll.item())\n",
    "    nlls[i] = nll\n",
    "\n",
    "print('==========')\n",
    "# same as loss\n",
    "print('average negative log likelihood, i.e. loss =', nlls.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "65f88321-9869-4acd-8717-fd752ff206df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cexze.\n",
      "momasurailezitynn.\n",
      "konimittain.\n",
      "llayn.\n",
      "ka.\n"
     ]
    }
   ],
   "source": [
    "# Sample from the neural net\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    out = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "        # ------------\n",
    "        # BEFORE:\n",
    "        # p = P[ix]\n",
    "        # ------------\n",
    "        # NOW:\n",
    "        xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float() # take ix, encode it into one-hot row of xenc\n",
    "        logits = xenc @ W # predict log-counts\n",
    "        counts = logits.exp() # counts, equivalent to N\n",
    "        p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "        # ------------\n",
    "\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "67931a4c-6a34-40d8-86bf-f0cdef1536ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our current \"neural net\" is very simple (the forward pass)\n",
    "# over time we will keep making that more and more complex until we get to transformers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

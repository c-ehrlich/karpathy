{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9626dffe-d4fe-4c1a-affa-977156760457",
   "metadata": {},
   "source": [
    "## Micrograd / Backpropagation\n",
    "\n",
    "What are neural nets\n",
    "- mathematical expressions\n",
    "- fairly simple in the case of MLP\n",
    "- take input:\n",
    "  - data\n",
    "  - weights&params\n",
    "- forward pass => loss function, which tries to measure the accuracy of the predictions, loss function is low when the NN is doing what you want it to do\n",
    "- use backpropagation to get the gradient, then we know how to tune all the parameters to decrease the loss locally\n",
    "- iterate that process many times: \"gradient descent\"\n",
    "- we have a blob of neural stuff (\"simulated neural tissue\") and we can make it do arbitrary things\n",
    "- these neural nets have fascinating emergent properties\n",
    "- this NN is 41 parameters. GPT is hundreds of billions, but works fundamentally in the same way.\n",
    "- real one:\n",
    "  - not mean squared error, but cross entropy loss (or max margin, binary cross entropy)\n",
    "  - use a different method of updating\n",
    "  - micrograd uses relu instead of tanh\n",
    "  - loss function usually only run on a subset of the data per pass\n",
    "  - \"L2 Regularization\" => prevents overfitting\n",
    "  - \"learning rate decay\" => start high, become lower over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "589058fe-907e-42f1-8189-0d347ebabdeb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.13/site-packages (2.2.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: matplotlib in /opt/homebrew/Cellar/python-matplotlib/3.10.1/libexec/lib/python3.13/site-packages (3.10.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/Cellar/python-matplotlib/3.10.1/libexec/lib/python3.13/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/Cellar/python-matplotlib/3.10.1/libexec/lib/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/Cellar/python-matplotlib/3.10.1/libexec/lib/python3.13/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/homebrew/Cellar/python-matplotlib/3.10.1/libexec/lib/python3.13/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/homebrew/lib/python3.13/site-packages (from matplotlib) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Cellar/jupyterlab/4.3.5_1/libexec/lib/python3.13/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/homebrew/lib/python3.13/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/homebrew/Cellar/python-matplotlib/3.10.1/libexec/lib/python3.13/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/homebrew/Cellar/jupyterlab/4.3.5_1/libexec/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Cellar/jupyterlab/4.3.5_1/libexec/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: graphviz in /opt/homebrew/Cellar/jupyterlab/4.3.5_1/libexec/lib/python3.13/site-packages (0.20.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torch in /opt/homebrew/Cellar/jupyterlab/4.3.5_1/libexec/lib/python3.13/site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in /opt/homebrew/Cellar/jupyterlab/4.3.5_1/libexec/lib/python3.13/site-packages (0.21.0)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Cellar/jupyterlab/4.3.5_1/libexec/lib/python3.13/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/homebrew/Cellar/jupyterlab/4.3.5_1/libexec/lib/python3.13/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/Cellar/jupyterlab/4.3.5_1/libexec/lib/python3.13/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Cellar/jupyterlab/4.3.5_1/libexec/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/Cellar/jupyterlab/4.3.5_1/libexec/lib/python3.13/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/Cellar/jupyterlab/4.3.5_1/libexec/lib/python3.13/site-packages (from torch) (75.8.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/homebrew/Cellar/jupyterlab/4.3.5_1/libexec/lib/python3.13/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/Cellar/jupyterlab/4.3.5_1/libexec/lib/python3.13/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.13/site-packages (from torchvision) (2.2.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/homebrew/lib/python3.13/site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Cellar/jupyterlab/4.3.5_1/libexec/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install matplotlib\n",
    "!{sys.executable} -m pip install graphviz\n",
    "!{sys.executable} -m pip install torch torchvision\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984eaa3d-1f81-4df9-a08b-ef7a9cc5c25c",
   "metadata": {},
   "source": [
    "## Value Class\n",
    "\n",
    "The point of this is that each value object tracks its **dependencies** (\"i was created by adding 2 + 5\"), and the **gradient** (\"the current slope of this dependency indicates that increasing it would have xyz change on me\", basically a derivative). We need this so that we can backtrack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "041ede0c-497c-49c9-8486-18f69b380ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wraps a value\n",
    "class Value:\n",
    "    # `()` is an empty tuple\n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        self.data = data\n",
    "        # derivative of L with respect to this value\n",
    "        # 0 means \"no effect\" (gradient 0)\n",
    "        self.grad = 0.0\n",
    "        # default to nothing, on a leaf node there is nothing to do\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children) # set is cheaper than tuple\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "\n",
    "        def _backward():\n",
    "            # every grad needs to be multiplied by out.grad to respect the chain rule\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    " \n",
    "        return out\n",
    "        \n",
    "    # 2.__dd__(Value(3.0)) doesn't work\n",
    "    # so python checks if Value has an `radd` that knows how to do it\n",
    "    # \"reverse add\"\n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "\n",
    "    def __neg__(self): # -self\n",
    "        return self * -1\n",
    "\n",
    "    def __sub__(self, other): # self - other\n",
    "        return self + (-other)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def __rmul__(self, other): \n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other): # self / other\n",
    "        return self * other**-1\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
    "        out = Value(self.data**other, (self,), f'**{other}')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other * self.data ** (other - 1) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "        \n",
    "    def exp(self):\n",
    "        x = self.data\n",
    "        out = Value(math.exp(x), (self, ), 'exp')\n",
    "\n",
    "        def _backward():\n",
    "            # derivative of e^x is e^x\n",
    "            self.grad += out.data * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    # we could implement `__exp__` instead... but want to show here\n",
    "    # that we can build functiond at arbitrary places in the complexity\n",
    "    # stack\n",
    "    # !! AS LONG AS WE KNOW THE LOCAL DERIVATIVE !!\n",
    "    def tanh(self):\n",
    "        n = self.data\n",
    "        t = (math.exp(2*n) - 1)/(math.exp(2*n) + 1)\n",
    "        out = Value(t, (self,), 'tanh')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1 - t**2) * out.grad\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                # each node only adds itself to the list\n",
    "                # after all of its children have been processed\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        \n",
    "        self.grad = 1.0\n",
    "\n",
    "        for node in reversed(topo):\n",
    "            node._backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa412438-7eac-43cf-bcf5-5dc9f44ee274",
   "metadata": {},
   "source": [
    "![Neuron](./_resources/neuron.png)\n",
    "\n",
    "A Neuron\n",
    "- Also called Perceptron, which is an artificial neuron / mathematical equation: `y = f(w1x1 + w2x2 + ... + wnxn + b)`\n",
    "- takes one or more numerical inputs (x1, x2, ..., xn)\n",
    "- multiplies each input by a weight (w1, w2, ..., wn), which are parameters\n",
    "- takes the sum of these x1w1 + x2w2 + ... + xnwn\n",
    "- adds the bias, which is the parameter\n",
    "- runs that through an activation function like tanh or relu\n",
    "- passes the outcome to one or more neurons in the next layer (this is called Multi Layer Perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26e140fc-8c8d-4a68-b8c9-90542ccbe393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a single neuron that subscribes to the pytorch api\n",
    "\n",
    "class Neuron:\n",
    "    # nin: number of inputs - w0x0, w1x1, etc\n",
    "    def __init__(self, nin):\n",
    "        # weight\n",
    "        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n",
    "        # bias\n",
    "        self.b = Value(random.uniform(-1,1))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # w * x + b\n",
    "        # multiply all the elements of w with all the elements of x, pair wise\n",
    "        # raw activation\n",
    "        act = sum(wi * xi for wi, xi in zip(self.w, x)) + self.b\n",
    "        # pass that through a nonlinearity\n",
    "        out = act.tanh()\n",
    "        return out\n",
    "\n",
    "    # same name as in pytorch\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0e4d15-f4f9-4949-9858-e187f61ecbe6",
   "metadata": {},
   "source": [
    "![Neural Net](./_resources/neural_net2.jpg)\n",
    "\n",
    "Real LLMs can have much larger output layers. Sometimes (eg binary classification), one neuron is enough.\n",
    "\n",
    "In the case of something like GPT-4o, there are thousands of neurons in the output layer, each one contains the likelihood of that token. ie for input \"the cat sat on the\":\n",
    "\n",
    "\"mat\" → 0.75, \"floor\" → 0.10, \"table\" → 0.05, \"moon\" → 0.01, ...\n",
    "\n",
    "Then it either picks the highest-probability token, or adds some randomness through techniques like temperature sampling\n",
    "\n",
    "If generating 100 tokens and it has a vocabulary of 100k, the output would be a 100 x 100k matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89ff4074-2e85-48a2-a14a-62dd586d22dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, nin, nout):\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs[0] if len(outs) == 1 else outs\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "        # long form version:\n",
    "        # params = []\n",
    "        # for neuron in self.neurons\n",
    "        #     ps = neuron.parameters()\n",
    "        #     params.extend(ps)\n",
    "        # return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e7ae017-4552-4418-8c85-fd5c163ebcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi layer perceptron\n",
    "class MLP:\n",
    "    # nouts is a list, defines the sizes of all the layers\n",
    "    def __init__(self, nin, nouts):\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff0b4375-7a9f-4d11-810d-9617a527c040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets [1.0, -1.0, -1.0, 1.0]\n",
      "step 1\n",
      "    curr: [-0.9270016790755088, -0.8928533274122414, -0.848556003804691, -0.9116328426552154]\n",
      "    loss: 7.402091289708123\n",
      "step 2\n",
      "    curr: [-0.8818482070283263, -0.8382348497473819, -0.7887388201806861, -0.8581389571556461]\n",
      "    loss: 7.064832308330099\n",
      "step 3\n",
      "    curr: [-0.7657897598497311, -0.7131426296153756, -0.6649362844291296, -0.7237808372088818]\n",
      "    loss: 6.283988695154863\n",
      "step 4\n",
      "    curr: [-0.39319773622744664, -0.3770748814142159, -0.3720276793003758, -0.3164821078196889]\n",
      "    loss: 4.456510011368637\n",
      "step 5\n",
      "    curr: [0.264256911208705, 0.11794358616453098, 0.023020382085314275, 0.3419865949516506]\n",
      "    loss: 3.2706680979358733\n",
      "step 6\n",
      "    curr: [0.08886614994099246, -0.19527525648188238, -0.2945303953362878, 0.22420666928171679]\n",
      "    loss: 2.577289460644992\n",
      "step 7\n",
      "    curr: [0.41547017612471754, -0.12164858290395585, -0.2894446358608733, 0.5477658610611773]\n",
      "    loss: 1.822580968842923\n",
      "step 8\n",
      "    curr: [0.39882822518762157, -0.4172037509907331, -0.5544753362786131, 0.5711210664277473]\n",
      "    loss: 1.083488336336464\n",
      "step 9\n",
      "    curr: [0.6479043229921385, -0.48878335070132584, -0.6133108476717424, 0.7590838197154778]\n",
      "    loss: 0.5928829347390188\n",
      "step 10\n",
      "    curr: [0.6984662667522914, -0.6347407243870484, -0.7131237590238673, 0.7972625705812372]\n",
      "    loss: 0.34773737363152246\n",
      "step 11\n",
      "    curr: [0.7582107540790863, -0.7029721130333088, -0.7584127019650251, 0.8363368471226803]\n",
      "    loss: 0.2318376552604863\n",
      "step 12\n",
      "    curr: [0.7953816891882635, -0.7484537115124047, -0.7894706633790801, 0.8601297626714437]\n",
      "    loss: 0.1690304732397242\n",
      "step 13\n",
      "    curr: [0.8210300316989614, -0.7800879416635172, -0.8117969619025369, 0.8764137615689709]\n",
      "    loss: 0.1310855048341097\n",
      "step 14\n",
      "    curr: [0.8398458106405159, -0.8033343032893261, -0.8286989330669742, 0.8883412553576253]\n",
      "    loss: 0.1061384914195923\n",
      "step 15\n",
      "    curr: [0.8542908095844057, -0.8211918605869806, -0.8420138648935543, 0.8975122470442262]\n",
      "    loss: 0.08866687728370953\n",
      "step 16\n",
      "    curr: [0.8657716005124384, -0.8353925315602728, -0.8528271762196025, 0.9048235587856782]\n",
      "    loss: 0.0758312769168474\n",
      "step 17\n",
      "    curr: [0.8751469080889349, -0.8469963809819073, -0.8618201211962555, 0.9108166427006315]\n",
      "    loss: 0.06604575211779086\n",
      "step 18\n",
      "    curr: [0.882970100915555, -0.8566866264351818, -0.869442555486696, 0.9158381485582808]\n",
      "    loss: 0.05836318387818698\n",
      "step 19\n",
      "    curr: [0.8896139591922604, -0.8649232520797383, -0.8760042043379604, 0.920120737661448]\n",
      "    loss: 0.05218645972753537\n",
      "step 20\n",
      "    curr: [0.8953390233633917, -0.8720275058676592, -0.8817258073171486, 0.9238267121527861]\n",
      "    loss: 0.04712203372121544\n"
     ]
    }
   ],
   "source": [
    "xs = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0],\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0] # desired targets\n",
    "\n",
    "n = MLP(3, [4,4,1])\n",
    "\n",
    "step = 0\n",
    "\n",
    "# gradient descent\n",
    "\n",
    "learning_rate = 0.05 # if we make too large of a step, we may overstep...\n",
    "\n",
    "print(\"targets\", ys)\n",
    "\n",
    "for k in range(20):\n",
    "    step += 1\n",
    "\n",
    "    # forward pass\n",
    "    ypred = [n(x) for x in xs]\n",
    "    loss = sum((yout-ygt)**2 for ygt, yout in zip(ys, ypred))\n",
    "\n",
    "    # backward pass\n",
    "    for p in n.parameters():\n",
    "        p.grad = 0.0  # Reset gradients to avoid accumulation (all the operations on p.grad are +=)\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    for p in n.parameters():\n",
    "        # -1 because we want to minimize the loss, not maximize it\n",
    "        # The gradient of a loss function L with respect to a parameter w (i.e., dL/dw)\n",
    "        # is a vector pointing in the direction that increases the loss the most.\n",
    "        # \"the gradient vectors are pointing in the direction of increasing the loss\"\n",
    "        p.data += learning_rate * p.grad * -1\n",
    "\n",
    "    print(\"step\", step)\n",
    "    print(\"    curr:\", [value.data for value in ypred])\n",
    "    print(\"    loss:\", loss.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
